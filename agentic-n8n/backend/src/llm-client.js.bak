// src/llm-client.js
const fetch = require('node-fetch');

const LOCAL_LLM_URL = process.env.LOCAL_LLM_URL || 'http://localhost:8000/generate';
const DEFAULT_TIMEOUT = parseInt(process.env.LOCALAI_TIMEOUT_MS || '60000', 10);

/**
 * callLocalLLM(prompt, options) -> normalized object
 * - tries POSTing to LOCAL_LLM_URL
 * - supports simple /generate or OpenAI-style /v1 endpoints
 * - returns parsed JSON if possible; otherwise { text: "<raw>" }
 */
async function callLocalLLM(prompt, options = {}) {
  const url = LOCAL_LLM_URL;
  const body = {};

  // If URL looks like OpenAI v1, try to send accordingly (flexible)
  if (url.includes('/v1/')) {
    // simple completions payload
    body.model = options.model || 'mock-model';
    // if it's chat completions style, send messages
    if (url.includes('/chat')) {
      body.messages = [{ role: 'user', content: prompt }];
    } else {
      body.prompt = prompt;
      body.max_tokens = options.maxTokens || 150;
    }
  } else {
    // legacy/mock endpoint
    body.prompt = prompt;
    body.options = options;
  }

  const controller = new AbortController();
  const timeout = setTimeout(() => controller.abort(), DEFAULT_TIMEOUT);

  try {
    const resp = await fetch(url, {
      method: 'POST',
      body: JSON.stringify(body),
      headers: { 'Content-Type': 'application/json' },
      signal: controller.signal
    });
    clearTimeout(timeout);
    const text = await resp.text();

    // Try JSON parse; if not possible, return text wrapper
    try {
      const parsed = JSON.parse(text);

      // Normalize common shapes: if OpenAI-like chat/completions, extract text
      if (parsed.choices && Array.isArray(parsed.choices) && parsed.choices.length) {
        const choice = parsed.choices[0];
        const content = (choice.message && choice.message.content) || choice.text || '';
        return { ...parsed, text: content };
      }

      // If parsed is an object with label/confidence, return as-is
      if (typeof parsed === 'object') {
        return parsed;
      }

      // otherwise wrap
      return { text: String(parsed) };
    } catch (e) {
      return { text };
    }
  } catch (err) {
    clearTimeout(timeout);
    return { error: 'llm_error', message: err?.message || String(err) };
  }
}

module.exports = { callLocalLLM };
