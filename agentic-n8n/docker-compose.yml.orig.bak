version: "3.8"

services:
  n8n:
    image: n8nio/n8n:latest
    restart: unless-stopped
    environment:
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=admin@n8n.local
      - N8N_BASIC_AUTH_PASSWORD=AgenticAI123
      - N8N_HOST=localhost
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
    ports:
      - "5678:5678"
    volumes:
      - ./n8n:/home/node/.n8n
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5678/healthz"]
      interval: 30s
      timeout: 10s
      retries: 5

  backend:
    build: ./backend
    restart: unless-stopped
    environment:
      - PORT=3100
      - N8N_WEBHOOK_URL=http://n8n:5678/webhook/chat-to-calendar
      - LOCALAI_URL=http://localai:8080
    ports:
      - "3100:3100"
    depends_on:
      - n8n
      - localai
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3100/ || exit 1"]
      interval: 20s
      timeout: 5s
      retries: 5

  frontend:
    build: ./frontend
    restart: unless-stopped
    environment:
      - VITE_BACKEND_URL=http://backend:3100
      - HOST=0.0.0.0
    ports:
      - "5173:5173"
    depends_on:
      - backend

  localai:
    image: localai/localai:latest
    container_name: localai
    restart: unless-stopped
    ports:
      - "8000:8080"                # Host 8000 -> container 8080 (LocalAI API)
    volumes:
      - ./localai-data:/data       # map your ./localai-data folder into the container
    environment:
      - MODELS_PATH=/data/models 
      - THREADS=8
    command: ["--address","0.0.0.0:8080"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 30s
      timeout: 10s
      retries: 3
